# Connect Four AI

This project implements and compares various AI approaches for playing Connect Four.

## Motivation

Recent advancements in Reinforcement Learning, such as AlphaGo and Noam Brown's work, have shown promising results by scaling inference compute to solve games. This project explores how far a naive, simple approach without human data can go in a relatively simple game like Connect Four. The goal is to use a small policy model (fully connected linear layers only, no convolutions) trained solely through self-play.

## Approaches and Challenges

### 1. Self-play

Initially, we aimed to train a policy model using only self-play. However, this approach faced significant challenges:

- Limited feedback from self-play
- Insufficient exploration, even with epsilon-greedy strategy
- Model vulnerability to simple strategies even after extensive training

**Future exploration:** The intuition remains that Connect Four's complexity should allow for relatively quick learning by a neural network. Further investigation into potential logic bugs is warranted.

### 2. Training against Minimax

To isolate whether the issue lies with self-play or model capacity, we implemented a minimax algorithm as an opponent.

### 3. Supervised Learning from Minimax Games

Current focus: Training the model on games generated by two minimax models playing against each other.

**Implementation note:** I reimplemented the minimax algorithm in Go which yielded very significant performance gains. Python's dynamic nature appears to introduce considerable overhead for an algorithm such as minimax.[^1]

[^1]: Reimplementing the minimax algorithm in Go led to very significant performance gains. The overhead of Python's dynamic typing shows very strongly with an algorithm like minimax. The algorithm would likely be even faster in C, using smart upfront stack allocations instead of heap memory for every new branch of the minimax tree. However, for practical purposes, the Go implementation is sufficiently fast. I'm trying very hard not to fall down another rabbit hole by thinking about a smart C implementation.

## Web Interface

This project includes a web-based interface for playing against the AI and visualizing the game. The interface allows users to:

- Choose between playing against the AI model or a Minimax algorithm
- Set the depth for the Minimax algorithm
- Play the game interactively
- View and replay saved games
- See game statistics

The web interface provides an intuitive way to interact with the AI and observe its performance in real-time.

## Research Question

The project aims to quantify the efficiency gap between our naive approach and more advanced techniques like those in AlphaGo. Formally, scale $p_n$ so that it's performance matches that of the AlphaGo approach, we seek to find $\Delta$ such that:

$$
    \varphi(p_n) + \Delta = \varphi(v_\theta) + \varphi(p_\rho) + \varphi(p_\pi)
$$

Where:
- $p_n$ is our naive model
- $\varphi$ maps a model to its parameter count
- $v_\theta$, $p_p$, and $p_\pi$ are as defined in the AlphaGo paper

## Next Steps

1. Debug and optimize the self-play approach
2. Implement and test advanced techniques from the AlphaGo paper
3. Conduct comparative analysis of parameter efficiency between approaches

## References

- Silver, D., et al. (2016). Mastering the game of Go with deep neural networks and tree search. *Nature*, 529(7587), 484-489.
